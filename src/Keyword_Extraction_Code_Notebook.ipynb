{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9300d3e3-99c5-4771-94c8-69509937bb4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24b7908c-d5a4-41e3-896a-5f32aaa4ee9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%pip install pandas fsspec adlfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7738e3bd-5ec7-4113-b08d-879c479d579d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f79092a-3bde-49be-82ca-c12a3334a1c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%run ./_paramgplayscrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a81f01f3-f33a-496a-bc31-7592c74bd3ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%run ./_envsettings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b69280aa-a7e5-4050-9a9d-9f9dfe2dc4c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from rake_nltk import Rake\n",
    "from keybert import KeyBERT\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0332dae9-83e3-4fbc-986f-fe0fc06cc6dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize KeyBERT\n",
    "kw_model = KeyBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8fba315-d34d-452e-81c8-3a282f06867f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize KeyBERT\n",
    "kw_model = KeyBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4a61832-a46e-4100-893d-528be427fa5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    f\"abfss://root@{account_name}.dfs.core.windows.net/{keyword_extraction_input_file}\",\n",
    "    storage_options={\n",
    "        \"account_name\": account_name,\n",
    "        \"account_key\": account_key\n",
    "    },encoding='ISO-8859-1', on_bad_lines='skip'\n",
    ") # Safe mode\n",
    "data = data.fillna(\"\")\n",
    "data = data.astype(str)\n",
    "# Ensure 'content' column exists\n",
    "# if 'review' not in data.columns:\n",
    "#     raise ValueError(\"The input file does not have a 'review' column.\")\n",
    "# Ensure 'content' column exists and rename it to 'review'\n",
    "if 'content' not in data.columns:\n",
    "    raise ValueError(\"The input file does not have a 'content' column.\")\n",
    "data = data.rename(columns={'content': 'review'})\n",
    "# Assign unique docIDs starting from 1\n",
    "data['docID'] = range(1, len(data) + 1)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fc4e028-c4d2-4590-91e7-d3a628034227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to calculate relevance/coherence score\n",
    "def calculate_relevance(text, keywords):\n",
    "    # Ensure text and keywords are non-empty\n",
    "    if not text.strip() or not keywords:\n",
    "        return {}\n",
    "\n",
    "    # Initialize TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit-transform the text and keywords\n",
    "    try:\n",
    "        vectors = vectorizer.fit_transform([text] + keywords)\n",
    "        text_vector = vectors[0]  # The vector for the text\n",
    "        keyword_vectors = vectors[1:]  # The vectors for the keywords\n",
    "        # Calculate cosine similarity\n",
    "        scores = cosine_similarity(text_vector, keyword_vectors).flatten()\n",
    "        return dict(zip(keywords, scores))\n",
    "    except ValueError as e:\n",
    "        # Handle empty or invalid vectors\n",
    "        print(f\"ValueError in calculate_relevance: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddf26e1b-6cac-4840-a535-c563b44d9b19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to clean and filter keywords\n",
    "def clean_keywords(keywords):\n",
    "    filtered_keywords = []\n",
    "    seen = set()\n",
    "    for kw in keywords:\n",
    "        kw = kw.strip().lower()\n",
    "        kw = re.sub(r'[^a-zA-Z\\s]', '', kw)  # Remove non-alphanumeric characters\n",
    "        if kw not in seen and len(kw.split()) > 1:  # Remove duplicates and single words\n",
    "            seen.add(kw)\n",
    "            filtered_keywords.append(kw)\n",
    "    return filtered_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd05d75a-e557-4e0d-9e27-bff65397313e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to detect redundancy using POS and regex\n",
    "def remove_redundancy(keywords):\n",
    "    final_keywords = []\n",
    "    for keyword in keywords:\n",
    "        doc = nlp(keyword)\n",
    "        # Include only noun phrases (e.g., NN, NNP)\n",
    "        if any(token.pos_ in ['NOUN', 'PROPN'] for token in doc):\n",
    "            final_keywords.append(keyword)\n",
    "    return list(set(final_keywords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed1cafd8-3bef-4f2d-8d84-e4cd57eb299d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to flatten nested lists\n",
    "def flatten_list(nested_list):\n",
    "    flat_list = []\n",
    "    for item in nested_list:\n",
    "        if isinstance(item, list):\n",
    "            flat_list.extend(flatten_list(item))  # Recursively flatten\n",
    "        else:\n",
    "            flat_list.append(item)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58a5e379-873e-48af-b92e-8d22c2cbf3b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the improved RAKE extraction function\n",
    "def extract_keyphrases_rake(text, num_phrases=5, min_length=2, max_length=5):\n",
    "    nltk.download(\"stopwords\")\n",
    "    nltk.download('punkt_tab')\n",
    "    rake = Rake()\n",
    "    rake.extract_keywords_from_text(text)\n",
    "    key_phrases = rake.get_ranked_phrases_with_scores()  # Get all ranked phrases\n",
    "\n",
    "    filtered_phrases = []\n",
    "    for kp in key_phrases:\n",
    "        Key_Phrase = kp[1]\n",
    "        if isinstance(Key_Phrase, str) and min_length <= len(Key_Phrase.split()) <= max_length:\n",
    "            filtered_phrases.append(Key_Phrase)\n",
    "    return filtered_phrases  # Return a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea5ab6ff-9e62-49f4-8742-3dbb3c3d4e7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to extract keywords using spaCy\n",
    "def extract_keywords_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    keywords = [chunk.text for chunk in doc.noun_chunks if len(chunk.text.split()) > 1]\n",
    "    cleaned_keywords = [clean_keywords(kp) for kp in keywords]\n",
    "    return cleaned_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26cf898a-6bcd-427f-bb33-b5cd3ebc98bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to extract keywords using KeyBERT\n",
    "def extract_keywords_keybert(text):\n",
    "    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(2, 4), stop_words=\"english\", top_n=5)\n",
    "    return [clean_keywords(kw[0]) for kw in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0287f83-4399-4d72-863c-97471c9c78fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3466f97-d4cd-4a56-8946-ae6b5f5d3ae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare output structure\n",
    "output_rows = []\n",
    "# Process each review content\n",
    "for _, row in data.iterrows():\n",
    "    doc_id = row['docID']\n",
    "    review = row['review']\n",
    "\n",
    "    if pd.isnull(review):  # Skip if the review content is null\n",
    "        continue\n",
    "\n",
    "    # Extract keywords using spaCy\n",
    "    spacy_keywords = extract_keywords_spacy(review)\n",
    "\n",
    "    # Extract keywords using RAKE\n",
    "    rake_keywords = extract_keyphrases_rake(review)\n",
    "\n",
    "    # Extract keywords using KeyBERT\n",
    "    keybert_keywords = extract_keywords_keybert(review)\n",
    "    \n",
    "    # Flatten and clean extracted keywords\n",
    "    spacy_keywords = flatten_list(spacy_keywords)\n",
    "    rake_keywords = flatten_list(rake_keywords)\n",
    "    keybert_keywords = flatten_list(keybert_keywords)\n",
    "    \n",
    "    # Ensure all elements are strings\n",
    "    spacy_keywords = [str(kw) for kw in spacy_keywords if kw]\n",
    "    rake_keywords = [str(kw) for kw in rake_keywords if kw]\n",
    "    keybert_keywords = [str(kw) for kw in keybert_keywords if kw]\n",
    "\n",
    "    # Combine all keywords into a set\n",
    "    all_keywords = set(spacy_keywords + rake_keywords + keybert_keywords)\n",
    "    \n",
    "    # Calculate relevance scores for cleaned keywords\n",
    "    cleaned_all_keywords = clean_keywords(list(all_keywords))  # Clean all keywords\n",
    "    relevance_scores = calculate_relevance(review, cleaned_all_keywords)\n",
    "    if not relevance_scores:\n",
    "        print(f\"Relevance scores could not be calculated for docID: {doc_id}\")\n",
    "        continue  # Skip to the next review if scores can't be calculated\n",
    "\n",
    "    # Weight and filter keywords\n",
    "    weighted_keywords = sorted(relevance_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Remove redundancy from cleaned keywords\n",
    "    final_keywords = remove_redundancy(cleaned_all_keywords)\n",
    "    for keyword in final_keywords:\n",
    "        try:\n",
    "            score = relevance_scores.get(keyword, 0)\n",
    "            output_rows.append({\n",
    "                'docID': doc_id,\n",
    "                'review': review,\n",
    "                'keyword': keyword,\n",
    "                'score': score\n",
    "            })\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError for keyword: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f3a976e-bd99-49f5-be35-beb43027e7ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(output_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caebf041-e93d-4b23-ac6b-f5362b48d0fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create output DataFrame\n",
    "output_df = pd.DataFrame(output_rows, columns=['docID', 'review', 'keyword', 'method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "540634c3-07c1-42a4-95d8-3d8e58a4df35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_df.to_csv(\n",
    "    f\"abfss://root@{account_name}.dfs.core.windows.net/{keyword_extraction_output_file}\",\n",
    "    index=False,\n",
    "    storage_options={\"account_name\": account_name, \"account_key\": account_key},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Keyword_Extraction_Code_Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
