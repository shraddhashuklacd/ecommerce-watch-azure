{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcbbaafe-bc2a-4a77-a4d0-5788c05e3997",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%run ./_paramgplayscrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0204952f-5ebd-4b89-a816-8918b1619b71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%run ./_envsettings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a037c5cc-3979-4291-87e4-97445d7ce702",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "171d65b1-d84c-4ef4-a7a7-d4537c46b8b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(f\"fs.azure.account.key.{account_name}.dfs.core.windows.net\", account_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba2f8ca1-7296-4124-bcde-2a28adc8438e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761993063092}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Writing data to parquest file in silver layer\n",
    "if keyword_extraction:\n",
    "    csv_path  = f\"abfss://root@{account_name}.dfs.core.windows.net/{keyword_extraction_output_file}\"\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"multiLine\", \"true\").option(\"escape\", '\"').option(\"quote\", '\"').schema(csv_keyword_schema).load(csv_path)\n",
    "    df = df.withColumn(\"etl_timestamp\", F.date_format(F.from_utc_timestamp(F.current_timestamp(), \"Asia/Kolkata\"),\"yyyy-MM-dd HH:mm:ss\"))\n",
    "    display(df)\n",
    "    delta_path = f\"abfss://root@{account_name}.dfs.core.windows.net/datalake/silver/{output_location}\"\n",
    "    df.write.format(\"delta\").mode(\"append\").save(delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae9416a9-1dc0-49ae-9a62-1445fb71df9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Writing data to parquest file in silver layer\n",
    "if sentiment_analyzer:\n",
    "    csv_path_sentiment  = f\"abfss://root@{account_name}.dfs.core.windows.net/{sentiment_analyzer_output_file}\"\n",
    "    df_sentiment = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"multiLine\", \"true\").option(\"escape\", '\"').option(\"quote\", '\"').schema(csv_schema).load(csv_path_sentiment)\n",
    "    df_sentiment_1 = ( df_sentiment.withColumn(\"date_proper\", F.date_format(F.col(\"Date\"), \"yyyy-MM-dd HH:mm:ss\")))\n",
    "    df_sentiment_2 = df_sentiment_1.withColumn(\"etl_timestamp\", F.date_format(F.from_utc_timestamp(F.current_timestamp(), \"Asia/Kolkata\"),\"yyyy-MM-dd HH:mm:ss\"))\n",
    "    #display(df_sentiment_2)\n",
    "    df_sentiment_final = df_sentiment_2.selectExpr(\"Review\",\"date_proper as Date\",\"Rating\",\"Document_ID\",\"Scores\",\"Sentiment\",\"etl_timestamp\")\n",
    "    #display(df_sentiment_final)\n",
    "    delta_path_sentiment = f\"abfss://root@{account_name}.dfs.core.windows.net/datalake/silver/{output_location_sentiment_analyzer}\"\n",
    "    df_sentiment_final.write.format(\"delta\").mode(\"append\").save(delta_path_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdabe17b-e574-4bce-af14-784c3403610a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762025305145}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Writing data to parquet file in silver layer\n",
    "if topic_modelling:\n",
    "    csv_path_topic = f\"abfss://root@{account_name}.dfs.core.windows.net/{topic_modelling_output_file}\"\n",
    "    df_topic = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"multiLine\", \"true\").option(\"escape\", '\"').option(\"quote\", '\"').schema(csv_topic_schema).load(csv_path_topic)\n",
    "    # Replace spaces and other invalid characters in column names\n",
    "    for col in df_topic.columns:\n",
    "        new_col = col.replace(\" \", \"_\")\n",
    "        if new_col != col:\n",
    "            df_topic = df_topic.withColumnRenamed(col, new_col)\n",
    "    display(df_topic)\n",
    "    df_topic = df_topic.withColumn(\"etl_timestamp\", F.date_format(F.from_utc_timestamp(F.current_timestamp(), \"Asia/Kolkata\"),\"yyyy-MM-dd HH:mm:ss\"))\n",
    "    delta_path_topic = f\"abfss://root@{account_name}.dfs.core.windows.net/datalake/silver/{output_location_topic_modelling}\"\n",
    "    df_topic.write.format(\"delta\").mode(\"append\").save(delta_path_topic)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "load-data-silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
